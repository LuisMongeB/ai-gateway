# ğŸš€ AI Gateway

A high-performance API gateway for LLM providers, built in Rust. Provides a unified OpenAI-compatible interface to multiple AI backends.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AI Gateway                              â”‚
â”‚                                                                 â”‚
â”‚   Client â”€â”€â–º /v1/chat/completions â”€â”€â”¬â”€â”€â–º Ollama (local)        â”‚
â”‚                                     â”‚                           â”‚
â”‚              OpenAI-compatible      â””â”€â”€â–º OpenAI API             â”‚
â”‚                                                                 â”‚
â”‚   Features: Auth â€¢ Rate Limiting â€¢ Streaming â€¢ Logging          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **ğŸ”„ Unified API** â€” OpenAI-compatible endpoint for all providers
- **ğŸŒŠ Streaming Support** â€” Server-Sent Events (SSE) for real-time responses
- **ğŸ”Œ Multiple Providers** â€” Ollama and OpenAI (more coming)
- **âš¡ High Performance** â€” Built with Rust + Actix-web for low latency
- **ğŸ”§ Easy Configuration** â€” Environment-based provider selection

## ğŸ—ï¸ Architecture

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    .env     â”‚
                         â”‚ AI_PROVIDER â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   AI Gateway    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Ollama    â”‚
â”‚        â”‚  OpenAI    â”‚                 â”‚  Ollama    â”‚  (local LLM) â”‚
â”‚        â”‚  Format    â”‚  â€¢ Auth         â”‚  Format    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚        â”‚            â”‚  â€¢ Rate Limit   â”‚
â”‚        â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â€¢ Transform    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        â”‚    SSE     â”‚  â€¢ Stream       â”‚  OpenAI    â”‚  OpenAI API  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Format    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Provider Comparison

| Aspect | Ollama | OpenAI |
|--------|--------|--------|
| Base URL | `localhost:11434` | `api.openai.com` |
| Auth | None (local) | Bearer token |
| Request Format | Transform needed | Pass-through |
| Streaming | NDJSON â†’ SSE | SSE (native) |

## ğŸš€ Getting Started

### Prerequisites

- [Rust](https://rustup.rs/) 1.75+
- [Ollama](https://ollama.ai/) (optional, for local models)
- OpenAI API key (optional, for OpenAI provider)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ai-gateway.git
cd ai-gateway

# Copy environment template
cp .env.example .env

# Build the project
cargo build --release
```

### Configuration

Edit `.env` to configure your gateway:

```bash
# Provider selection: "ollama" or "openai"
AI_PROVIDER=ollama

# Ollama configuration
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI configuration
OPENAI_API_KEY=sk-your-key-here
OPENAI_BASE_URL=https://api.openai.com
```

### Running

```bash
# Development
RUST_LOG=info cargo run

# Production
RUST_LOG=info ./target/release/ai-gateway
```

The gateway starts at `http://localhost:8080`

## ğŸ“¡ API Usage

### Chat Completions

```bash
# Non-streaming request
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
```

```bash
# Streaming request
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Tell me a story"}
    ],
    "stream": true
  }'
```

### Health Check

```bash
curl http://localhost:8080/health
```

### Response Format

Responses follow the OpenAI Chat Completions format:

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1704380400,
  "model": "gpt-4o-mini",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! I'm doing well, thank you for asking."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 12,
    "completion_tokens": 15,
    "total_tokens": 27
  }
}
```

## ğŸ“ Project Structure

```
ai-gateway/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ .env                  # Environment configuration
â””â”€â”€ src/
    â”œâ”€â”€ main.rs           # Server setup, provider selection
    â”œâ”€â”€ handlers/
    â”‚   â”œâ”€â”€ mod.rs
    â”‚   â””â”€â”€ chat.rs       # /v1/chat/completions endpoint
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ mod.rs        # Request/response structs
    â”œâ”€â”€ middleware/       # Auth, rate limiting (coming soon)
    â””â”€â”€ providers/
        â”œâ”€â”€ mod.rs        # LLMProvider trait, ProviderError
        â”œâ”€â”€ ollama.rs     # Ollama provider
        â””â”€â”€ openai.rs     # OpenAI provider
```

## ğŸ›£ï¸ Roadmap

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Basic proxy to Ollama | âœ… Complete |
| 2 | SSE streaming support | âœ… Complete |
| 3 | Multiple providers (Ollama + OpenAI) | âœ… Complete |
| 4 | Middleware (auth, rate limiting) | ğŸ”„ In Progress |
| 5 | Resilience (fallbacks, caching, retries) | â³ Planned |
| 6 | Azure OpenAI support | â³ Planned |

## ğŸ› ï¸ Tech Stack

- **Language:** Rust ğŸ¦€
- **Web Framework:** [Actix-web](https://actix.rs/) 4.x
- **HTTP Client:** [Reqwest](https://docs.rs/reqwest) with streaming
- **Async Runtime:** [Tokio](https://tokio.rs/)
- **Serialization:** [Serde](https://serde.rs/)

## ğŸ“„ License

Apache-2.0

---

<p align="center">
  Built with ğŸ¦€ and â˜•
</p>
